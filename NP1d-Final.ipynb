{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"g4cHz9c0MfIa"},"outputs":[],"source":["# Run in Colab GPU instance (e.g. T4) as it is needed to load models fitted with a GPU (NP bug?)\n","!pip uninstall -y torch notebook notebook_shim tensorflow tensorflow-datasets prophet torchaudio torchdata torchtext torchvision\n","!pip install git+https://github.com/ourownstory/neural_prophet.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awCYMb2tMqeN"},"outputs":[],"source":["import logging\n","import warnings\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import matplotlib as mpl\n","from matplotlib import gridspec\n","import pickle\n","import pandas as pd\n","import pickle\n","import os\n","import datetime\n","from tqdm.auto import tqdm\n","\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","# from prophet import Prophet\n","from neuralprophet import NeuralProphet, set_log_level\n","\n","set_log_level(\"ERROR\")\n","logging.getLogger(\"prophet\").setLevel(logging.ERROR)\n","warnings.filterwarnings(\"ignore\")\n","\n","# MASE:  Mean Absolute Scaled Error compares the mean absolute error of the forecast with\n","# the mean absolute error of a naive forecast, i.e. the forecast that simply repeats the last observed value.\n","\n","def mase(y_test,y_pred,y_train):\n","    return (y_test-y_pred.reset_index(drop=True)).abs().mean()/y_train.diff().dropna().abs().mean()\n"]},{"cell_type":"code","source":["# Connecting to Google Drive and mount project to running instance\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"A0nD7UAJvkg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LkGj3ICM84q"},"outputs":[],"source":["# Need to manually set project path.\n","fpath='/content/drive/MyDrive/Colab Notebooks/Capstone'\n","\n","print(f\"Project filepath, fpath: {fpath}\")\n","\n","# Load cleaned and reformated sales data\n","with open(fpath+'/data/df_v_m2.pkl', 'rb') as f:\n","    ddict=pickle.load(f)\n","    df_v_m=ddict['df_v_m']\n","    df_v_m_test=ddict['df_v_m_test']\n","# read price data\n","df_p_m=pd.read_pickle(fpath+'/data/df_flat_pvp.pkl')\n","df_p_m_test=pd.read_pickle(fpath+'/data/df_flat_pvp_test.pkl')\n","df_p_m_test[\"MED_GOA\"]=df_p_m_test.loc[:,[c for c in df_p_m_test.columns if 'GOA' in c]].median(axis=1)\n","df_p_m_test[\"MED_95\"]=df_p_m_test.loc[:,[c for c in df_p_m_test.columns if '95' in c]].median(axis=1)\n","\n","# nearest station info for each station (3&8 have same prices so use 2nd nearest for them)\n","stn_near=pd.read_pickle(fpath+'/data/stn_near.pkl')\n","\n","# read holidays defined in data and test period\n","df_hol_base=pd.read_pickle(fpath+'/data/df_hol_base.pkl')\n","\n","# add extra holidays/events to the base holidays\n","# Neuralprophet uses only 'event' not both 'holiday' and 'event' as prophet\n","# Defined as days that appear repeatedly in the time series as low sales days but are not official holidays\n","# are assumed to be foreseeable events that can be used to improve the forecast\n","with open(fpath+'/data/df_holi_extra.pkl', 'rb') as f:\n","    dict_hol_extra=pickle.load(f)\n","\n","\n","# dicts to store model performance statistics for later comparison\n","dict_MASE={}\n","dict_MAE={}\n"]},{"cell_type":"markdown","metadata":{"id":"vAb8IZd7mqGd"},"source":["# 1. Seasonalities only"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-PPvNGeVLig"},"outputs":[],"source":["dict_M = {}\n","valid = False\n","modelnr=1\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","# for stn in ['ES1_95']:\n","    # no extra holidays for ALL or 95 sales\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\": # seeing the effects of extra holidays if it improves the forecast and worth the effort\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        if df_hol_extra.loc[df_hol_extra.ds==datetime.datetime(2018,1,5)].shape[0]==1: #add extra event for 2019-01-04 if 2018-01-05 is present\n","            df_hol_extra=pd.concat([df_hol_extra,pd.DataFrame({'ds':[datetime.datetime(2019,1,4)],'event':['Ev01']},index=[0])],ignore_index=True)\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","    else:\n","        df_hol=df_hol_base\n","\n","    # read sales data for station\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","\n","    m=NeuralProphet(\n","                    yearly_seasonality=True,\n","                    weekly_seasonality=True,\n","                    daily_seasonality=False,\n","                    n_forecasts=1,\n","                    quantiles=[0.05,0.95],\n","                    epochs=60,\n","                    )\n","\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","    m.set_plotting_backend(\"matplotlib\")\n","\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","\n","\n","    # plot residuals\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","    fig=plt.figure(figsize=(18, 5))\n","\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    # Forecast test period\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol),\n","                                     periods=1, n_historic_predictions=31*4)\n","\n","    forecast_test = m.predict(future).dropna()\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    #m.highlight_nth_step_ahead_of_each_forecast_test(1).plot(forecast_test_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","\n","    plt.close()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wS3mIQiZmqGm"},"source":["# 2. One (1) day ahead forecast with Auto-Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtGkR-0OmqGm"},"outputs":[],"source":["valid=False\n","dict_M = {}\n","modelnr=2\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","\n","    # no extra holidays for ALL or 95 sales\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\":\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        if df_hol_extra.loc[df_hol_extra.ds==datetime.datetime(2018,1,5)].shape[0]==1: #add extra event for 2019-01-04 if 2018-01-05 is present\n","            df_hol_extra=pd.concat([df_hol_extra,pd.DataFrame({'ds':[datetime.datetime(2019,1,4)],'event':['Ev01']},index=[0])],ignore_index=True)\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","    else:\n","        df_hol=df_hol_base\n","\n","\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","\n","    m=NeuralProphet(\n","                yearly_seasonality=True,\n","                weekly_seasonality=True,\n","                daily_seasonality=False,\n","                seasonality_mode=\"additive\",\n","                n_lags=7,\n","                ar_reg=.1,\n","                n_forecasts=1,\n","                quantiles=[0.05,0.95],\n","                #epochs=60,\n","                )\n","\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","\n","    # plot residuals\n","    fig=plt.figure(figsize=(18, 5))\n","\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    # Forecast test period\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol), df_hol,\n","                                     periods=1, n_historic_predictions=31*4)\n","\n","    forecast_test = m.predict(future).dropna()\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    #m.highlight_nth_step_ahead_of_each_forecast_test(1).plot(forecast_test_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"kSAqgQmNmqGn"},"source":["# 3 One (1) ahead forecast modeling AR with a Neural Network (AR-Net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZ0IzBKQmqGn"},"outputs":[],"source":["valid=False\n","dict_M = {}\n","modelnr=3\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\":\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","    else:\n","        df_hol=df_hol_base\n","\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","    nl=8\n","    valid=False\n","\n","    m=NeuralProphet(\n","            yearly_seasonality=True,\n","            weekly_seasonality=True,\n","            daily_seasonality=False,\n","            seasonality_mode=\"additive\",\n","            n_lags=7,\n","            n_forecasts=1,\n","            quantiles=[0.05,0.95],\n","            ar_layers=[nl, nl, nl, nl],\n","            ar_reg=10,\n","            # learning_rate=0.003,\n","            )\n","\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","    m.set_plotting_backend(\"matplotlib\")\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","\n","    # plot residuals\n","    fig=plt.figure(figsize=(18, 5))\n","\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    # Forecast test period\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol),df_hol, periods=1, n_historic_predictions=31*4)\n","    forecast_test = m.predict(future).dropna()\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","\n","\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"YFku3ZNkmqGo"},"source":["# 4 AR-Net + meteo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9UUNBZtmqGo"},"outputs":[],"source":["dict_M={}\n","valid=False\n","modelnr=4\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\":\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        if df_hol_extra.loc[df_hol_extra.ds==datetime.datetime(2018,1,5)].shape[0]==1: #add extra event for 2019-01-04 if 2018-01-05 is present\n","            df_hol_extra=pd.concat([df_hol_extra,pd.DataFrame({'ds':[datetime.datetime(2019,1,4)],'event':['Ev01']},index=[0])],ignore_index=True)\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","    else:\n","        df_hol=df_hol_base\n","\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","    nl=8\n","    valid=False\n","\n","    # add weather data, temp, precipitation, wind\n","    df[\"tmed\"] = df_v_m.tmed.values\n","    df[\"prec\"] = df_v_m.prec.values\n","    df[\"velmedia\"] = df_v_m.velmedia.values\n","    df[\"sol\"] = df_v_m.sol.values\n","    df[\"racha\"] = df_v_m.racha.values\n","\n","    df_test[\"tmed\"] = df_v_m_test.tmed.values\n","    df_test[\"prec\"] = df_v_m_test.prec.values\n","    df_test[\"velmedia\"] = df_v_m_test.velmedia.values\n","    df_test[\"sol\"] = df_v_m_test.sol.values\n","    df_test[\"racha\"]= df_v_m_test.racha.values\n","\n","    m=NeuralProphet(\n","            yearly_seasonality=True,\n","            weekly_seasonality=True,\n","            daily_seasonality=False,\n","            seasonality_mode=\"additive\",\n","            n_lags=7,\n","            n_forecasts=1,\n","            quantiles=[0.05,0.95],\n","            ar_layers=[nl, nl, nl, nl],\n","            ar_reg=10,\n","            # learning_rate=0.003,\n","            )\n","\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    # add weather data as future regressors and assume reliable weather forecast is available for 1 day ahead\n","    m.add_future_regressor(\"tmed\",normalize=\"standardize\")\n","    m.add_future_regressor(\"prec\",normalize=\"standardize\")\n","    m.add_future_regressor(\"velmedia\",normalize=\"standardize\")\n","    m.add_future_regressor(\"sol\",normalize=\"standardize\")\n","    m.add_future_regressor(\"racha\",normalize=\"standardize\")\n","\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","    m.set_plotting_backend(\"matplotlib\")\n","\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    forecast_train = m.predict(df_new)\n","\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","\n","    # plot residuals\n","    fig=plt.figure(figsize=(18, 5))\n","\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    # Forecast test period\n","    # future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol),df_hol, periods=1, n_historic_predictions=31*4)\n","\n","    regressors_list=['tmed','prec','velmedia','sol','racha',]\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol),\n","                                    regressors_df=m.create_df_with_events(pd.concat([df,df_test]),df_hol).loc[:,regressors_list],\n","                                    periods=1, n_historic_predictions=31*14)\n","\n","    forecast_test = m.predict(future).dropna()\n","\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"4qpjOZufmqGo"},"source":["# 5 One (1) step ahead forecast using Auto-Regression $+\\Delta y$ lagged regressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOPMT5LamqGo"},"outputs":[],"source":["valid=False\n","dict_M={}\n","modelnr=5\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","\n","    # no extra holidays for ALL or 95 sales\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\":\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        if df_hol_extra.loc[df_hol_extra.ds==datetime.datetime(2018,1,5)].shape[0]==1: #add extra event for 2019-01-04 if 2018-01-05 is present\n","            df_hol_extra=pd.concat([df_hol_extra,pd.DataFrame({'ds':[datetime.datetime(2019,1,4)],'event':['Ev01']},index=[0])],ignore_index=True)\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","    else:\n","        df_hol=df_hol_base\n","\n","\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","\n","    lagg_I=pd.concat([df,df_test],ignore_index=True).y.diff().fillna(0)\n","\n","    #df2=df.copy(deep=True)\n","    df[\"I\"] = lagg_I.iloc[:len(df)].values\n","\n","    #df_test2=df_test.copy(deep=True)\n","    df_test[\"I\"]=lagg_I.iloc[len(df):].values\n","\n","\n","    m=NeuralProphet(\n","                    yearly_seasonality=True,\n","                    weekly_seasonality=True,\n","                    daily_seasonality=False,\n","                    seasonality_mode=\"additive\",\n","                    n_lags=7,\n","                    ar_reg=1,\n","                    n_forecasts=1,\n","                    quantiles=[0.05,0.95],\n","                    #epochs=60,\n","                    )\n","\n","    m.add_lagged_regressor(\"I\",normalize=\"standardize\",n_lags=7)\n","\n","\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","    m.set_plotting_backend(\"matplotlib\")\n","\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    # plot residuals\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","\n","    fig=plt.figure(figsize=(18, 5))\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","    plt.close()\n","\n","    # Forecast test period\n","    # Forecast test period\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol), events_df=df_hol,\n","                        regressors_df=pd.DataFrame({'ds':np.append(df_new.ds.values,df_test.ds.values),'I':lagg_I.values}).reset_index(drop=True),\n","                                     periods=1, n_historic_predictions=31*4)\n","\n","    forecast_test = m.predict(future).dropna()\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    #m.highlight_nth_step_ahead_of_each_forecast_test(1).plot(forecast_test_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"I6DWpTqXmqGp"},"source":["# 6 One (1) step ahead forecast using AR, $+\\Delta y$ lagged regressor and meteo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygbq94GYmqGp"},"outputs":[],"source":["dict_M={}\n","valid=False\n","modelnr=6\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","# for stn in ['ES10_95']:\n","    # add extra holidays/events\n","    # with open(fpath+'/df_holi_extra.pkl', 'rb') as f:\n","    #     dict_hol_extra=pickle.load(f)\n","\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\":\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        df_hol_extra=pd.concat([df_hol_extra,pd.DataFrame({'ds':[datetime.datetime(2019,1,4)],'event':['Ev01']},index=[0])],ignore_index=True)\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","        print(df_hol_extra)\n","    else:\n","        df_hol=df_hol_base\n","\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","    # add lagged regressor with difference of y\n","    # do the difference on concatenated df and df_test for continuity\n","    lagg_I=pd.concat([df,df_test],ignore_index=True).y.diff().fillna(0)\n","\n","    df[\"I\"] = lagg_I.iloc[:len(df)].values\n","    df_test[\"I\"]=lagg_I.iloc[len(df):].values\n","\n","    # add weather data, temp, precipitation, wind\n","    # df[\"tmax\"] = df_v_m.tmax.values\n","    df[\"tmed\"] = df_v_m.tmed.values\n","    # df[\"tmin\"] = df_v_m.tmin.values\n","    df[\"prec\"] = df_v_m.prec.values\n","    df[\"velmedia\"] = df_v_m.velmedia.values\n","    df[\"sol\"] = df_v_m.sol.values\n","    df[\"racha\"] = df_v_m.racha.values\n","\n","    # df_test[\"tmax\"] = df_v_m_test.tmax.values\n","    df_test[\"tmed\"] = df_v_m_test.tmax.values\n","    # df_test[\"tmin\"] = df_v_m_test.tmax.values\n","    df_test[\"prec\"] = df_v_m_test.prec.values\n","    df_test[\"velmedia\"] = df_v_m_test.velmedia.values\n","    df_test[\"sol\"] = df_v_m_test.sol.values\n","    df_test[\"racha\"] = df_v_m_test.racha.values\n","\n","    m=NeuralProphet(\n","                    yearly_seasonality=True,\n","                    weekly_seasonality=True,\n","                    daily_seasonality=False,\n","                    seasonality_mode=\"additive\",\n","                    n_lags=7,\n","                    ar_reg=0.1,\n","                    n_forecasts=1,\n","                    quantiles=[0.05,0.95],\n","                    #epochs=60,\n","                    )\n","\n","    # add holidays\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    # add lagged regressor\n","    m.add_lagged_regressor(\"I\",normalize=\"standardize\",n_lags=7,regularization=0.1)\n","\n","    # add weather data as future regressors and assume reliable weather forecast is available for 1 day ahead\n","    # m.add_future_regressor(\"tmax\",normalize=\"standardize\")\n","    m.add_future_regressor(\"tmed\",normalize=\"standardize\")\n","    # m.add_future_regressor(\"tmin\",normalize=\"standardize\")\n","    m.add_future_regressor(\"prec\",normalize=\"standardize\")\n","    m.add_future_regressor(\"velmedia\",normalize=\"standardize\")\n","    m.add_future_regressor(\"sol\",normalize=\"standardize\")\n","    m.add_future_regressor(\"racha\",normalize=\"standardize\")\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","    m.set_plotting_backend(\"matplotlib\")\n","\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","\n","    # plot residuals\n","    fig=plt.figure(figsize=(18, 5))\n","\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","\n","    plt.close()\n","\n","    # Forecast test period\n","    regressors_list=['I','tmed','prec','velmedia','sol','racha']\n","    #regressors_list=['I','tmax','tmed','tmin','prec','velmedia','sol','racha']\n","\n","\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol), events_df=df_hol,\n","                                regressors_df=pd.concat([df,df_test]).loc[:,regressors_list],\n","                                periods=1, n_historic_predictions=31*4)\n","\n","\n","    forecast_test = m.predict(future).dropna()\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"SuyTMd1DeHHV"},"source":["## 7. Model 6 with added PVP deviation from median as lagged regressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkzJ6Qo7eHHV"},"outputs":[],"source":["dict_M={}\n","valid=False\n","modelnr=7\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","# for stn in ['ES5_95']:\n","    # add extra holidays/events\n","    # with open(fpath+'/df_holi_extra.pkl', 'rb') as f:\n","    #     dict_hol_extra=pickle.load(f)\n","\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\":\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        df_hol_extra=pd.concat([df_hol_extra,pd.DataFrame({'ds':[datetime.datetime(2019,1,4)],'event':['Ev01']},index=[0])],ignore_index=True)\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","        print(df_hol_extra)\n","    else:\n","        df_hol=df_hol_base\n","\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","    # add lagged regressor with difference of y\n","    # do the difference on concatenated df and df_test for continuity\n","    lagg_I=pd.concat([df,df_test],ignore_index=True).y.diff().fillna(0)\n","\n","    df[\"I\"] = lagg_I.iloc[:len(df)].values\n","    df_test[\"I\"]=lagg_I.iloc[len(df):].values\n","\n","    # add weather data, temp, precipitation, wind\n","    # df[\"tmax\"] = df_v_m.tmax.values\n","    df[\"tmed\"] = df_v_m.tmed.values\n","    # df[\"tmin\"] = df_v_m.tmin.values\n","    df[\"prec\"] = df_v_m.prec.values\n","    df[\"velmedia\"] = df_v_m.velmedia.values\n","    df[\"sol\"] = df_v_m.sol.values\n","    df[\"racha\"] = df_v_m.racha.values\n","\n","    # add price deviation from median for this station\n","    df[\"pvpdev\"]=(df_p_m[stn]-df_p_m[\"MED_\"+stn.split('_')[1]]).values\n","\n","    # df_test[\"tmax\"] = df_v_m_test.tmax.values\n","    df_test[\"tmed\"] = df_v_m_test.tmax.values\n","    # df_test[\"tmin\"] = df_v_m_test.tmax.values\n","    df_test[\"prec\"] = df_v_m_test.prec.values\n","    df_test[\"velmedia\"] = df_v_m_test.velmedia.values\n","    df_test[\"sol\"] = df_v_m_test.sol.values\n","    df_test[\"racha\"] = df_v_m_test.racha.values\n","\n","    # add price deviation from median for this station\n","    df_test[\"pvpdev\"]=(df_p_m_test[stn]-df_p_m_test[\"MED_\"+stn.split('_')[1]]).values\n","\n","    m=NeuralProphet(\n","                    yearly_seasonality=True,\n","                    weekly_seasonality=True,\n","                    daily_seasonality=False,\n","                    seasonality_mode=\"additive\",\n","                    n_lags=7,\n","                    ar_reg=0.1,\n","                    n_forecasts=1,\n","                    quantiles=[0.05,0.95],\n","                    #epochs=60,\n","                    )\n","\n","    # add holidays\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    # add lagged regressor\n","    m.add_lagged_regressor(\"I\",normalize=\"standardize\",n_lags=7,regularization=0.1)\n","    m.add_lagged_regressor(\"pvpdev\",normalize=\"standardize\") #can add lags later\n","\n","    # add weather data as future regressors and assume reliable weather forecast is available for 1 day ahead\n","    # m.add_future_regressor(\"tmax\",normalize=\"standardize\")\n","    m.add_future_regressor(\"tmed\",normalize=\"standardize\")\n","    # m.add_future_regressor(\"tmin\",normalize=\"standardize\")\n","    m.add_future_regressor(\"prec\",normalize=\"standardize\")\n","    m.add_future_regressor(\"velmedia\",normalize=\"standardize\")\n","    m.add_future_regressor(\"sol\",normalize=\"standardize\")\n","    m.add_future_regressor(\"racha\",normalize=\"standardize\")\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","    m.set_plotting_backend(\"matplotlib\")\n","\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","\n","    # plot residuals\n","    fig=plt.figure(figsize=(18, 5))\n","\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","\n","    plt.close()\n","\n","    # Forecast test period\n","    regressors_list=['I','pvpdev','tmed','prec','velmedia','sol','racha']\n","    #regressors_list=['I','tmax','tmed','tmin','prec','velmedia','sol','racha']\n","\n","\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol), events_df=df_hol,\n","                                regressors_df=pd.concat([df,df_test]).loc[:,regressors_list],\n","                                periods=1, n_historic_predictions=31*4)\n","\n","\n","    forecast_test = m.predict(future).dropna()\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"uXA61RLfeHHW"},"source":["## 8. Model 7 with added PVP deviation of nearest station"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kf0cJZGaeHHW"},"outputs":[],"source":["dict_M={}\n","valid=False\n","modelnr=8\n","fit_model=False  # True to fit model, False to load fitted model from disk\n","\n","fpathm = fpath+f\"/m_{modelnr}\"\n","if not os.path.exists(fpathm):\n","    os.mkdir(fpathm)\n","\n","for stn in tqdm([f\"ES{nr}_{pr}\" for nr in range(1,13) for pr in [\"95\",\"GOA\"]]):\n","#for stn in ['ES5_95']:\n","    # add extra holidays/events\n","    # with open(fpath+'/df_holi_extra.pkl', 'rb') as f:\n","    #     dict_hol_extra=pickle.load(f)\n","\n","    if stn[:2]=='ES' and stn[-3:]==\"GOA\":\n","        df_hol_extra=dict_hol_extra[f\"df_holi_extra_{stn}\"]\n","        df_hol_extra=pd.concat([df_hol_extra,pd.DataFrame({'ds':[datetime.datetime(2019,1,4)],'event':['Ev01']},index=[0])],ignore_index=True)\n","        df_hol=pd.concat([df_hol_base,df_hol_extra],ignore_index=True)\n","        print(f\"Added {df_hol_extra.shape[0]} extra holidays for\",stn)\n","        print(df_hol_extra)\n","    else:\n","        df_hol=df_hol_base\n","\n","    df=df_v_m.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","    df_test=df_v_m_test.loc[:,[stn]].reset_index().rename(columns={'sale_date':'ds',stn:'y'})\n","\n","    # add lagged regressor with difference of y\n","    # do the difference on concatenated df and df_test for continuity\n","    lagg_I=pd.concat([df,df_test],ignore_index=True).y.diff().fillna(0)\n","\n","    df[\"I\"] = lagg_I.iloc[:len(df)].values\n","    df_test[\"I\"]=lagg_I.iloc[len(df):].values\n","\n","    # add weather data, temp, precipitation, wind\n","    # df[\"tmax\"] = df_v_m.tmax.values\n","    df[\"tmed\"] = df_v_m.tmed.values\n","    # df[\"tmin\"] = df_v_m.tmin.values\n","    df[\"prec\"] = df_v_m.prec.values\n","    df[\"velmedia\"] = df_v_m.velmedia.values\n","    df[\"sol\"] = df_v_m.sol.values\n","    df[\"racha\"] = df_v_m.racha.values\n","\n","    # add price deviation from median for this station\n","    df[\"pvpdev\"]=(df_p_m[stn]-df_p_m[\"MED_\"+stn.split('_')[1]]).values\n","    df[\"pvpdev_n\"]=(df_p_m[stn_near.loc[stn.split('_')[0]].Nearest+'_'+stn.split('_')[1]]\\\n","                  -df_p_m[\"MED_\"+stn.split('_')[1]]).values\n","\n","    # df_test[\"tmax\"] = df_v_m_test.tmax.values\n","    df_test[\"tmed\"] = df_v_m_test.tmax.values\n","    # df_test[\"tmin\"] = df_v_m_test.tmax.values\n","    df_test[\"prec\"] = df_v_m_test.prec.values\n","    df_test[\"velmedia\"] = df_v_m_test.velmedia.values\n","    df_test[\"sol\"] = df_v_m_test.sol.values\n","    df_test[\"racha\"] = df_v_m_test.racha.values\n","\n","    # add price deviation from median for this station\n","    df_test[\"pvpdev\"]=(df_p_m_test[stn]-df_p_m_test[\"MED_\"+stn.split('_')[1]]).values\n","    df_test[\"pvpdev_n\"]=(df_p_m_test[stn_near.loc[stn.split('_')[0]].Nearest+'_'+stn.split('_')[1]]\\\n","                  -df_p_m_test[\"MED_\"+stn.split('_')[1]]).values\n","\n","    m=NeuralProphet(\n","                    yearly_seasonality=True,\n","                    weekly_seasonality=True,\n","                    daily_seasonality=False,\n","                    seasonality_mode=\"additive\",\n","                    n_lags=7,\n","                    ar_reg=0.1,\n","                    n_forecasts=1,\n","                    quantiles=[0.05,0.95],\n","                    #epochs=60,\n","                    )\n","\n","    # add holidays\n","    for e in df_hol.event.unique():\n","      m.add_events(e,regularization=None,mode='additive')#,lower_window=0,upper_window=1)\n","\n","    # add lagged regressor\n","    m.add_lagged_regressor(\"I\",normalize=\"standardize\",n_lags=7,regularization=0.1)\n","    m.add_lagged_regressor(\"pvpdev\",normalize=\"standardize\") #can add lags later\n","    m.add_lagged_regressor(\"pvpdev_n\",normalize=\"standardize\") #can add lags later\n","\n","    # add weather data as future regressors and assume reliable weather forecast is available for 1 day ahead\n","    # m.add_future_regressor(\"tmax\",normalize=\"standardize\")\n","    m.add_future_regressor(\"tmed\",normalize=\"standardize\")\n","    # m.add_future_regressor(\"tmin\",normalize=\"standardize\")\n","    m.add_future_regressor(\"prec\",normalize=\"standardize\")\n","    m.add_future_regressor(\"velmedia\",normalize=\"standardize\")\n","    m.add_future_regressor(\"sol\",normalize=\"standardize\")\n","    m.add_future_regressor(\"racha\",normalize=\"standardize\")\n","\n","    df_new=m.create_df_with_events(df, df_hol)\n","\n","    if fit_model:\n","       if valid:\n","          df_train, df_val = m.split_df(df_new, freq=\"D\", valid_p=0.1)\n","          metrics=m.fit(df_train,freq='D',validation_df=df_val, progress='bar')\n","       else:\n","          df_train=df_new\n","          metrics=m.fit(df_train,freq='D', progress='bar')\n","       print(metrics.tail(1))\n","\n","      # save fitted model to disk\n","       with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"wb\") as f:\n","          pickle.dump(m, f, pickle.HIGHEST_PROTOCOL)\n","    else:\n","        print(\"Loading model from file: \"+fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl')\n","        with open(fpath+f'/models_1d/{stn}_model_{modelnr}_model.pkl', \"rb\") as f:\n","          m=pickle.load(f)\n","\n","    m.set_plotting_backend(\"matplotlib\")\n","\n","    forecast_train = m.predict(df_new)\n","    forecast_train.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    # plot forecast in training period\n","    fig, ax = plt.subplots(figsize=(18, 6))\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_train, xlabel=\"Date\", ylabel=stn, ax=ax)\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} Train Forecast\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_train_forecast.png\",bbox_inches='tight')\n","    plt.close()\n","    df_residuals = pd.DataFrame({\"ds\": forecast_train.ds, \"residuals\": forecast_train.y - forecast_train.yhat1})\n","\n","    # plot residuals\n","    fig=plt.figure(figsize=(18, 5))\n","\n","    spec = gridspec.GridSpec(ncols=2, nrows=1,\n","                            width_ratios=[3, 1], wspace=0.08)\n","\n","    ax0=fig.add_subplot(spec[0])\n","    df_residuals.plot(x=\"ds\", y=\"residuals\",ax=ax0)\n","    ax0.set_title(\"Residuals\")\n","    ax0.grid()\n","\n","    ax1=fig.add_subplot(spec[1])\n","    ax1.set_title(\"Residuals histogram\")\n","    df_residuals.residuals.hist(bins=50,ax=ax1,orientation='horizontal')\n","    ax1.set_yticklabels([])\n","\n","    fig.suptitle(f\"Model {modelnr}: {stn} Residuals\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_residuals.png\",bbox_inches='tight')\n","\n","    plt.close()\n","\n","    # Forecast test period\n","    regressors_list=['I','pvpdev','pvpdev_n','tmed','prec','velmedia','sol','racha']\n","\n","\n","    future = m.make_future_dataframe(m.create_df_with_events(pd.concat([df,df_test]),df_hol), events_df=df_hol,\n","                                regressors_df=pd.concat([df,df_test]).loc[:,regressors_list],\n","                                periods=1, n_historic_predictions=31*4)\n","\n","\n","    forecast_test = m.predict(future).dropna()\n","\n","    fig,ax=plt.subplots(1,1,figsize=(18,5))\n","\n","    m.highlight_nth_step_ahead_of_each_forecast(1).plot(forecast_test,ax=ax)\n","    ax.set_xlim(pd.Timestamp('2018-10-01'),pd.Timestamp('2019-01-31'))\n","\n","    t_start_test='2019-01-01'\n","    forecast_test['residuals']=forecast_test.y-forecast_test.yhat1\n","\n","    forecast_test.set_index('ds',inplace=True)\n","\n","    forecast_test.to_pickle(fpathm+f\"/{stn}_forecast_test.pkl\")\n","\n","    ax.plot(df_test.ds,df_test.y,'rx',label='test')\n","    ax.legend(['yhat','yhat','test','test'])\n","\n","    mase_test =mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index<t_start_test,'y'])\n","    mase_test2=mase(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1'],forecast_test.loc[forecast_test.index>=t_start_test,'y'])\n","\n","    rmse_test=np.sqrt(mean_squared_error(df_test.y,forecast_test.loc[forecast_test.index>=t_start_test,'yhat1']))\n","\n","    mae_test=forecast_test.loc[forecast_test.index>=t_start_test,'residuals'].abs().mean()\n","    mae_test_rel=mae_test/forecast_test.loc[forecast_test.index>=t_start_test,'y'].mean()\n","    mae_train=df_residuals.residuals.abs().mean()\n","\n","    print(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%,MASE2={mase_test2*100:.1f}%, MAE={mae_test:.4f}, MAE_rel={mae_test_rel*100:.1f}%\")\n","\n","    dict_MASE[(stn,modelnr)]=mase_test\n","    dict_MAE[(stn,modelnr)]=mae_test_rel\n","    dict_M[(stn,modelnr)]={'mase_test':mase_test,'mase_test2':mase_test2,'mae_test':mae_test,'rmse_test':rmse_test,'mae_test_rel':mae_test_rel,'mae_train':mae_train}\n","\n","    ax.set_title(f\"Model {modelnr}: {stn} MASE={mase_test*100:.1f}%, MAE_rel={mae_test_rel*100:.1f}%\")\n","    fig.savefig(fpathm+f\"/{stn}_model_{modelnr}_forecast.png\",bbox_inches='tight')\n","\n","    with open(fpathm+f'/dict_MASE_MAE_{modelnr}.pkl', 'wb') as f:\n","      pickle.dump({'dict_MASE':dict_MASE,'dict_MAE':dict_MAE,'dict_M':dict_M}, f, pickle.HIGHEST_PROTOCOL)\n","\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"g_XsU8TJeHHW"},"source":["# Appendix: plotting components of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgQ6fIbkmqGp"},"outputs":[],"source":["\n","# use most recent results to plot\n","fc=m.predict(df_new,decompose=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oWMnbipeHHW"},"outputs":[],"source":["plt.rcParams.update({'axes.titlesize': 'large',\n","                     'axes.labelsize':'x-large',\n","                     'xtick.labelsize':'x-large',\n","                     'ytick.labelsize':'x-large'})\n","\n","fig,ax=plt.subplots(3,1,figsize=(16,16))\n","fig.tight_layout(h_pad=10)\n","\n","# trend\n","fc.set_index('ds').trend.plot(ax=ax[0],grid=True,lw=2,xlabel='',ylabel='Sales')\n","ax[0].set_title('Trend',fontsize=16)\n","\n","# weekly seasonality\n","weekdays = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n","wkday={i:weekdays[i] for i in range(7)}\n","\n","wk=fc.groupby(fc.ds.dt.weekday).season_weekly.max().reset_index()\n","wk['weekday']=wk.ds.map(wkday)\n","wk.set_index('weekday',inplace=True)\n","wk.drop('ds',axis=1,inplace=True)\n","wk.plot(kind='bar',ax=ax[1],grid=True,ylim=(-0.4,0.4),legend=False,xlabel='',alpha=0.5,width=1.0)\n","wk.plot(kind='line',ax=ax[1],grid=True,ylim=(-0.4,0.4),legend=False,xlabel='',lw=2,drawstyle='steps-mid')\n","for label in ax[1].get_xticklabels():\n","    label.set_ha(\"right\")\n","    label.set_rotation(45)\n","ax[1].set_title('Seasonality by weekday',fontsize=16)\n","\n","# yearly seasonality (day of year)\n","an=fc.groupby(fc.ds.dt.dayofyear).season_yearly.mean()\n","an=an.to_frame().set_index(pd.date_range(start='2020-01-01',end='2020-12-31',freq='d'))\n","an.plot.area(ax=ax[2],stacked=False,grid=True,ylim=(-0.4,0.4),legend=False)\n","ax[2].set_title('Seasonality by day of year',fontsize=16)\n","ax[2].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec','Jan']);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awFliFPfeHHW"},"outputs":[],"source":["\n","plt.rcParams.update({'axes.titlesize': 'large',\n","                    'axes.labelsize':'medium',\n","                    'xtick.labelsize':'medium',\n","                    'ytick.labelsize':'medium'})\n","\n","\n","m.plot_parameters(components=['events','autoregression','lagged_regressors','future_regressors'],plotting_backend='matplotlib')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOdYQVsQeHHX"},"outputs":[],"source":["plt.rcParams.update({'axes.titlesize': 'x-large',\n","                     'axes.labelsize':'x-large',\n","                     'xtick.labelsize':'x-large',\n","                     'ytick.labelsize':'x-large'})\n","\n","from sklearn import linear_model\n","fig,ax=plt.subplots(5,1,figsize=(16,16),layout='tight')\n","\n","fx=fc.copy(deep=True)\n","lm=linear_model.LinearRegression().fit(fx.index.values.reshape(-1, 1),fx.ar1.interpolate(method='bfill').values.reshape(-1, 1))\n","\n","fx['trend_2']=fx.trend+lm.predict(fx.index.values.reshape(-1, 1)).reshape(-1)\n","fx['ar1_2']=fx.ar1-lm.predict(fx.index.values.reshape(-1, 1)).reshape(-1)\n","\n","fx['s2']=fx.trend_2+fx.season_yearly\n","fx['s3']=fx.s2+fx.season_weekly+fx.ar1_2+fx.lagged_regressor_I1\n","fx['s4']=fx.s3+fx.events_additive\n","fx['s5']=fx.s4+fx.future_regressors_additive#+fx.lagged_regressor_I1+fx.ar1_2\n","\n","mae_trend=(fx.trend_2-fx.y).abs().mean()/fx.y.mean()\n","mae_s2=(fx.s2-fx.y).abs().mean()/fx.y.mean()\n","mae_s3=(fx.s3-fx.y).abs().mean()/fx.y.mean()\n","mae_s4=(fx.s4-fx.y).abs().mean()/fx.y.mean()\n","mae_s5=(fx.s5-fx.y).abs().mean()/fx.y.mean()\n","\n","\n","xlims=(pd.Timestamp('2018-01-01'),pd.Timestamp('2018-12-31'))\n","ylims=(0,1.0)\n","\n","fx.set_index('ds').trend_2.plot(ax=ax[0],grid=True,lw=2,xlabel='',ylabel='Sales',ylim=(0,0.2),c='C1')\n","fx.set_index('ds').y.plot(ax=ax[0],alpha=0.25,color='C0',grid=True,xlabel='')\n","fx.set_index('ds').yhat1.plot(ax=ax[0],grid=True,lw=1,ls='--'\n","                              ,xlabel='',ylabel='Sales',ylim=ylims,c='C2')\n","ax[0].set_xlim(xlims)\n","ax[0].set_title(f'Trend + Autoregression (MAE={mae_trend*100:.1f}%)',fontsize=16)\n","\n","\n","fx.set_index('ds').s2.plot(ax=ax[1],grid=True,lw=2,xlabel='',ylabel='Sales',ylim=ylims,c='C1')\n","fx.set_index('ds').y.plot(ax=ax[1],alpha=0.25,color='C0',grid=True,xlabel='')\n","fx.set_index('ds').yhat1.plot(ax=ax[1],grid=True,lw=1,ls='--'\n","                              ,xlabel='',ylabel='Sales',ylim=ylims,c='C2')\n","ax[1].set_xlim(xlims)\n","ax[1].set_title(f'+ Seasonality yearly (MAE={mae_s2*100:.1f}%)',fontsize=16)\n","\n","fx.set_index('ds').s3.plot(ax=ax[2],grid=True,lw=2,xlabel='',ylabel='Sales',ylim=ylims,c='C1')\n","fx.set_index('ds').y.plot(ax=ax[2],alpha=0.25,color='C0',grid=True,xlabel='')\n","fx.set_index('ds').yhat1.plot(ax=ax[2],grid=True,lw=1,ls='--'\n","                              ,xlabel='',ylabel='Sales',ylim=ylims,c='C2')\n","ax[2].set_xlim(xlims)\n","ax[2].set_title(f'+ Seasonality weekly (MAE={mae_s3*100:.1f}%)',fontsize=16)\n","\n","fx.set_index('ds').s4.plot(ax=ax[3],grid=True,lw=2,xlabel='',ylabel='Sales',ylim=ylims,c='C1')\n","fx.set_index('ds').y.plot(ax=ax[3],alpha=0.25,color='C0',grid=True,xlabel='')\n","fx.set_index('ds').yhat1.plot(ax=ax[3],grid=True,lw=1,ls='--'\n","                              ,xlabel='',ylabel='Sales',ylim=ylims,c='C2')\n","\n","ax[3].set_xlim(xlims)\n","ax[3].set_title(f'+ Events (MAE={mae_s4*100:.1f}%)',fontsize=16)\n","\n","fx.set_index('ds').s5.plot(ax=ax[4],grid=True,lw=2,xlabel='',ylabel='Sales',ylim=ylims,c='C1')\n","fx.set_index('ds').y.plot(ax=ax[4],alpha=0.25,color='C0',grid=True,xlabel='')\n","fx.set_index('ds').yhat1.plot(ax=ax[4],grid=True,lw=1,ls='--'\n","                              ,xlabel='',ylabel='Sales',ylim=ylims,c='C2')\n","ax[4].set_title(f'+ Regressors (future+lagged+auto) = Total Forecast (MAE={mae_s5*100:.1f}%)',fontsize=16)\n","ax[4].set_xlim(xlims)\n","\n","\n","# fx.set_index('ds').ar1_2.plot(ax=ax[1],grid=True,lw=2,xlabel='',ylabel='Sales',title='Autoregression',ylim=(0,1))\n","\n","\n","fig.suptitle('Components of forecast for ES12_GOA (Training)',fontsize=16,y=1.0);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owTvq4ekeHHX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}